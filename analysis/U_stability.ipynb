{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3e7b70",
   "metadata": {},
   "source": [
    "# Test Stability of U \n",
    "#### (Using the Tonsil dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b046f9",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "First, we import the neccessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bcfe9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import anndata as ad\n",
    "from pathlib import Path\n",
    "import errno\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151797d",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Next, we import some helper functions including the cisi segementation training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a914bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper fncs\n",
    "import helpers.analysis_utils\n",
    "\n",
    "\n",
    "## CISI\n",
    "# Import system libraries to configure code directory as module\n",
    "from os.path import dirname, abspath, join\n",
    "import sys\n",
    "\n",
    "# Find code directory relative to our directory\n",
    "THIS_DIR = dirname('__file__')\n",
    "CODE_DIR = abspath(join(THIS_DIR, '..', 'code'))\n",
    "# Add code directory to systems paths\n",
    "sys.path.append(CODE_DIR)\n",
    "\n",
    "# Import dictionary training fnc. (smaf)\n",
    "from compute_dictionary import smaf\n",
    "\n",
    "\n",
    "# Define fnc to compare correlation matrices\n",
    "def compare_cor(A, B):\n",
    "    return np.sum(abs(A - B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7124f068",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "In the first part we specify the paths to the input files (.h5ad files created from R) and where the outputs should be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b81cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input paths\n",
    "training_data_path = Path('/mnt/bb_dqbm_volume')\n",
    "#spe_path = Path(os.path.join(training_data_path, \n",
    "#                             'data/Tonsil_th152/preprocessed_data/spe.h5ad'))\n",
    "spe_path = Path(os.path.join(training_data_path, \n",
    "                             'data/Immucan_lung/Lung_sce.h5ad'))\n",
    "data_name = 'Immucan_lung'\n",
    "\n",
    "# Specify output path\n",
    "out_path = Path(os.path.join(training_data_path, \n",
    "                             'analysis', data_name, 'tests/test_U_stability'))\n",
    "out_path_k = Path(os.path.join(training_data_path, \n",
    "                               'analysis', data_name, 'tests/test_U_stability_par/k'))\n",
    "out_path_maxItr = Path(os.path.join(training_data_path, \n",
    "                               'analysis', data_name, 'tests/test_U_stability_par/maxItr'))\n",
    "out_path_norm = Path(os.path.join(training_data_path, \n",
    "                               'analysis', data_name, 'tests/test_U_stability_par/transformation'))\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "out_path.mkdir(parents=True, exist_ok=True)\n",
    "out_path_k.mkdir(parents=True, exist_ok=True)\n",
    "out_path_maxItr.mkdir(parents=True, exist_ok=True)\n",
    "out_path_norm.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aefbb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that input files/dictionary exist\n",
    "if not helpers.analysis_utils.is_valid_file(spe_path, ['.h5ad']):\n",
    "    # If file is not found, throw error\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT),\n",
    "                            spe_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b54eb",
   "metadata": {},
   "source": [
    "Next, we read in the input files for training U and testing its stability. For this we have the Tonsil th152 dataset consisting of 5 ROIs and we read the data in once it has been processed by steinbock into segmented single cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82f3e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View of AnnData object with n_obs × n_vars = 2322277 × 36\n",
      "    obs: 'sample_id', 'ObjectNumber', 'Pos_X', 'Pos_Y', 'area', 'major_axis_length', 'minor_axis_length', 'eccentricity', 'width_px', 'height_px', 'acquisition_id', 'image', 'Slide_id', 'slide_position', 'sample_name', 'ROI', 'sample_type', 'subbatch', 'cell_id', 'cell_labels', 'celltype', 'celltype_minor', 'TLS', 'distance_to_TLS', 'Tumor_mask', 'distance_to_Tumor', 'CN_30_15', 'CN_minor_30_15', 'clusters'\n",
      "    var: 'channel', 'name', 'keep', 'ilastik', 'deepcell', 'Tube.Number', 'Target', 'Antibody.Clone', 'Stock.Concentration', 'Final.Concentration...Dilution', 'uL.to.add', 'tumorMask', 'channel_name'\n",
      "    uns: 'X_name'\n",
      "    layers: 'exprs', 'normalized', 'scaled'\n"
     ]
    }
   ],
   "source": [
    "# Read in SpatialExperiment converted to anndata by cellconverter in R\n",
    "spe = ad.read_h5ad(spe_path)\n",
    "spe = spe[:, ~spe.var.index.str.contains('Histone|Ir[0-9]|E-Cad', regex=True, case=False)]\n",
    "print(spe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945a683",
   "metadata": {},
   "source": [
    "Next, we specify the parameters we want to use to compute the dictionary and how many times to assess it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae4ee4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set non-default parameters of smaf\n",
    "d = 80\n",
    "lda1 = 3\n",
    "lda2 = 0.2\n",
    "\n",
    "# Set parameters for stability analysis of U\n",
    "n_sizes = 10\n",
    "n_rep = 10\n",
    "min_train_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847478b",
   "metadata": {},
   "source": [
    "## Check different Training Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a9a53",
   "metadata": {},
   "source": [
    "In the following paragraphs, we compute the \"ground truth\" correlation between proteins in U by computing the correlation matrix of U of the full dataset n_rep times and taking its average. Then we compare this to the results of input data sets of different sizes (by taking the sum of the absolut differences between ground truth and each result)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7985dbb7",
   "metadata": {},
   "source": [
    "### k = 3 (paper default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c06904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate \"ground truth\" by computing the average correlation matrix of the full dataset\n",
    "cor_list = [-10.0] * n_rep \n",
    "for i in range(n_rep):\n",
    "        U, W = smaf(spe, d, lda1, lda2, normalization='paper_norm', layer=None,\n",
    "                    outpath=os.path.join(out_path, \"k_3\", str(spe.shape[0]), str(i)))\n",
    "        cor_w_diag = np.corrcoef(U)\n",
    "        # Remove diagonal\n",
    "        cor_list[i] = cor_w_diag[~np.eye(len(cor_w_diag), dtype=bool)].reshape(len(cor_w_diag), -1)\n",
    "\n",
    "# Calculate \"ground truth\" as average of the five runs of the full data set\n",
    "cor_ground_truth = sum(cor_list) / n_rep\n",
    "\n",
    "\n",
    "# Initialize list of lists to hold results of distances between correlation matrices\n",
    "# cor_list_subset = [-10] * 6\n",
    "# cor_list_subset[5] = [compare_cor(c, cor_ground_truth) for c in pages]\n",
    "cor_list_subset = np.full((n_rep, n_sizes), 10.0)\n",
    "cor_list_subset[:, (n_sizes-1)] = np.array([compare_cor(c, cor_ground_truth) for c in cor_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate correlation matrix of the subsets of the dataset\n",
    "# For different subset sizes of spe, caculate n_rep times U and compare how close its\n",
    "# columnwise correlations are to the mean correlation matrix of the full dataset (excluding)\n",
    "# the diagonal\n",
    "j = 0\n",
    "for s in np.linspace(min_train_size, spe.shape[0], n_sizes-1, dtype=int, endpoint=False):\n",
    "    for i in range(n_rep):\n",
    "        X_subset = spe[np.random.randint(spe.shape[0], size=(s)), ]\n",
    "        \n",
    "        U, W = smaf(X_subset, d, lda1, lda2, outpath=os.path.join(out_path, \"k_3\", \n",
    "                                                                  str(s), str(i)), \n",
    "                    normalization='paper_norm', layer=None)\n",
    "        cor_w_diag = np.corrcoef(U)\n",
    "        cor_wt_diag = cor_w_diag[~np.eye(len(cor_w_diag), dtype=bool)].reshape(len(cor_w_diag), -1)\n",
    "        \n",
    "        cor_list_subset[i, j] = compare_cor(cor_wt_diag, cor_ground_truth)\n",
    "    \n",
    "    j+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198dfd4",
   "metadata": {},
   "source": [
    "### k = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eaac81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set k to 1\n",
    "lda1 = 1\n",
    "\n",
    "# Calculate \"ground truth\" by computing the average correlation matrix of the full dataset\n",
    "cor_list_k1 = [-10.0] * n_rep \n",
    "for i in range(n_rep):\n",
    "        U, W = smaf(spe, d, lda1, lda2, normalization='paper_norm', layer=None,\n",
    "                    outpath=os.path.join(out_path, \"k_1\", str(spe.shape[0]), str(i)))\n",
    "        cor_w_diag = np.corrcoef(U)\n",
    "        # Remove diagonal\n",
    "        cor_list_k1[i] = cor_w_diag[~np.eye(len(cor_w_diag), dtype=bool)].reshape(len(cor_w_diag), -1)\n",
    "\n",
    "# Calculate \"ground truth\" as average of the five runs of the full data set\n",
    "cor_ground_truth_k1 = sum(cor_list_k1) / n_rep\n",
    "\n",
    "\n",
    "# Initialize list of lists to hold results of distances between correlation matrices\n",
    "# cor_list_subset = [-10] * 6\n",
    "# cor_list_subset[5] = [compare_cor(c, cor_ground_truth) for c in pages]\n",
    "cor_list_subset_k1 = np.full((n_rep, n_sizes), 10.0)\n",
    "cor_list_subset_k1[:, (n_sizes-1)] = np.array([compare_cor(c, cor_ground_truth_k1) for c in cor_list_k1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate correlation matrix of the subsets of the dataset\n",
    "# For different subset sizes of spe, caculate n_rep times U and compare how close its\n",
    "# columnwise correlations are to the mean correlation matrix of the full dataset (excluding)\n",
    "# the diagonal\n",
    "j = 0\n",
    "for s in np.linspace(min_train_size, spe.shape[0], n_sizes-1, dtype=int, endpoint=False):\n",
    "    for i in range(n_rep):\n",
    "        X_subset = spe[np.random.randint(spe.shape[0], size=(s)), ]\n",
    "        \n",
    "        U, W = smaf(X_subset, d, lda1, lda2, outpath=os.path.join(out_path, \"k_1\", str(s), \n",
    "                                                                  str(i)), \n",
    "                    normalization='paper_norm', layer=None)\n",
    "        cor_w_diag = np.corrcoef(U)\n",
    "        cor_wt_diag = cor_w_diag[~np.eye(len(cor_w_diag), dtype=bool)].reshape(len(cor_w_diag), -1)\n",
    "        \n",
    "        cor_list_subset_k1[i, j] = compare_cor(cor_wt_diag, cor_ground_truth_k1)\n",
    "    \n",
    "    j+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0809a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into pandas dataframe for easier handling\n",
    "cor_pd = pd.DataFrame(cor_list_subset, \n",
    "                         columns=np.append(np.linspace(min_train_size, spe.shape[0], \n",
    "                                                       n_sizes-1, dtype=int, endpoint=False), \n",
    "                                           spe.shape[0]))\n",
    "cor_pd_k1 = pd.DataFrame(cor_list_subset_k1, \n",
    "                         columns=np.append(np.linspace(min_train_size, spe.shape[0], \n",
    "                                                       n_sizes-1, dtype=int, endpoint=False), \n",
    "                                           spe.shape[0]))\n",
    "# Melt dataframe into long format for plotting\n",
    "# cor_pd_long = pd.melt(cor_pd, var_name='subset_size', value_name='distance')\n",
    "\n",
    "# Use seaborns regression fnc of order 2 to plot results\n",
    "# ax = sns.regplot(x=\"subset_size\", y=\"distance\", data=cor_pd_long, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278e1f7",
   "metadata": {},
   "source": [
    "## Check different Parameters\n",
    "### k-sparsity (lda1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431204fd",
   "metadata": {},
   "source": [
    "Next, we have a look at how the dictionary U changes, for different values (1-10) of k (columnwise-sparsity constraint of W used when computing U in smaf()). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify list of k to try (1 to 10)\n",
    "k = np.arange(1, 10+1)\n",
    "for i in k:\n",
    "    for j in range(n_rep):\n",
    "        U, W = smaf(spe, d, i, lda2, normalization='paper_norm', layer=None,\n",
    "                    outpath=os.path.join(out_path_k, str(i), str(j)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed91ae",
   "metadata": {},
   "source": [
    "### maxItr (number of iterations to compute U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b92239",
   "metadata": {},
   "source": [
    "Next, we have a look at how the dictionary U changes, if we change the number of iterations to compute U. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set k back to 3\n",
    "lda1 = 3\n",
    "\n",
    "# Specify list of itr to try (10 to 50)\n",
    "itr = np.arange(10, 50+1, 10)\n",
    "for i in itr:\n",
    "    for j in range(n_rep):\n",
    "        U, W = smaf(spe, d, lda1, lda2, maxItr=i, normalization='paper_norm', layer=None,\n",
    "                    outpath=os.path.join(out_path_maxItr, str(i), str(j)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ef3c4a",
   "metadata": {},
   "source": [
    "### normalization (X normalization type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd3ab8",
   "metadata": {},
   "source": [
    "Next, we have a look at how the dictionary U changes, if we change the type of normalization of X before training U. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set k back to 1\n",
    "lda1 = 1\n",
    "\n",
    "# Specify list of itr to try (10 to 50)\n",
    "itr = ['paper_norm', 'min_max_norm', 'none']\n",
    "for i in itr:\n",
    "    for j in range(n_rep):\n",
    "        U, W = smaf(spe, d, lda1, lda2, maxItr=10, normalization=i, layer=None,\n",
    "                    outpath=os.path.join(out_path_norm, str(i), str(j)))\n",
    "        \n",
    "        \n",
    "itr = ['exprs', 'log_exprs']\n",
    "for i in itr:\n",
    "    for j in range(n_rep):\n",
    "        U, W = smaf(spe, d, lda1, lda2, maxItr=10, normalization='none', layer=i,\n",
    "                    outpath=os.path.join(out_path_norm, str(i), str(j)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06bb9f2",
   "metadata": {},
   "source": [
    "Next, we have a look at how the dictionary U changes, if we change the type of normalization of X before training U. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10951c",
   "metadata": {},
   "source": [
    "### dictionary size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd443c29",
   "metadata": {},
   "source": [
    "Next, we have a look at how the dictionary U changes, if we change the upper number of modules (dictionary size) to train U for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23291a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify list of # modules in dictionary from 10 to 80\n",
    "dict_sizes = np.linspace(10, 100, 10, dtype=int, endpoint=True)\n",
    "for i in dict_sizes:\n",
    "    for j in range(n_rep):\n",
    "        U, W = smaf(spe, i, lda1, lda2, maxItr=10, normalization='paper_norm', layer=None,\n",
    "                    outpath=os.path.join(out_path_norm, str(i), str(j)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7833e32",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e92358",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_pd.to_csv(os.path.join(out_path, \"k_3\", \"results.csv\"))\n",
    "cor_pd_k1.to_csv(os.path.join(out_path, \"k_1\", \"results.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cisi_imc",
   "language": "python",
   "name": "cisi_imc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
